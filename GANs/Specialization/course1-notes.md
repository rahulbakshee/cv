## Discriminator:
The discriminator is a type of classifier that learns to model the probability of an example being real or fake given that set of input features, like RGB pixel values for images.The output probabilities from the discriminator are the ones that help the generator learn to produce better looking examples overtime.


<img src="https://render.githubusercontent.com/render/math?math=P(y | X)"> or <img src="https://render.githubusercontent.com/render/math?math=P(class | features)">

The y(class) can be “real” or “fake” here. X is an image generated by Generator.


## Generators:
The generator produces fake data that tries to look real. It learns to mimic that distribution of features X from the class of your data. In order to produce different outputs each time it takes random features(noise) as input.


<img src="https://render.githubusercontent.com/render/math?math=P(X | y)"> or <img src="https://render.githubusercontent.com/render/math?math=P(features | class)">

If we have only one class(y) , then the second term in the conditional probability will be ignored(coz it will be same always) and we will only have P(X). If we want to model the probabilities across all classes then we will have to put the second term.


The more common features(in the dataset) will be appearing more, and rare features will appear rare.

## Binary CrossEntropy(BCE):
The BCE cost function has two main terms that are relevant for each of the classes. Whether prediction and the label are similar, the BCE loss is close to 0. When they're very different, that BCE loss approaches infinity. The BCE loss is performed across a mini-batch of several examples, let say n examples, maybe five examples where n equals 5. It takes the average of all those five examples. Each of those examples can be different. One of them can be 1, the other four could be 0, for their different classes.
