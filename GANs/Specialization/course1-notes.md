## Discriminator
The discriminator is a type of classifier that learns to model the probability of an example being real or fake given that set of input features, like RGB pixel values for images.The output probabilities from the discriminator are the ones that help the generator learn to produce better looking examples overtime.


<img src="https://render.githubusercontent.com/render/math?math=P(y | X)"> or <img src="https://render.githubusercontent.com/render/math?math=P(class | features)">

The y(class) can be “real” or “fake” here. X is an image generated by Generator.


## Generators
The generator produces fake data that tries to look real. It learns to mimic that distribution of features X from the class of your data. In order to produce different outputs each time it takes random features(noise) as input.


<img src="https://render.githubusercontent.com/render/math?math=P(X | y)"> or <img src="https://render.githubusercontent.com/render/math?math=P(features | class)">

If we have only one class(y) , then the second term in the conditional probability will be ignored(coz it will be same always) and we will only have P(X). If we want to model the probabilities across all classes then we will have to put the second term.


The more common features(in the dataset) will be appearing more, and rare features will appear rare.

## Binary CrossEntropy(BCE)
The BCE cost function has two main terms that are relevant for each of the classes. Whether prediction and the label are similar, the BCE loss is close to 0. When they're very different, that BCE loss approaches infinity. The BCE loss is performed across a mini-batch of several examples, let say n examples, maybe five examples where n equals 5. It takes the average of all those five examples. Each of those examples can be different. One of them can be 1, the other four could be 0, for their different classes.


## training GANs : Discriminator
So to train a basic GAN, you alternate the training on the generator and the discriminator. So first of course you get some fake examples X hat produced by the generator from that input noise. And then those examples, the fake ones, X hat and real ones X are both passed into the discriminator without telling the discriminator just yet which ones are real and which ones are fake. And then the discriminator makes predictions Y hat of which ones are real and which ones are fake. Or more specifically a probability of score of how fake and how real each of these images are. After that, the predictions are compared using that BCE loss with the desired labels for fake and real. And that helps update its parameters or theta d, where d represents parameters for the discriminator. And so this only updates the parameters of the discriminator, only this one neural network, and not the generator.

## training GANs : Generator
so for the generator it first generates a few fake examples, again X hat, and this is from the input noise. And then these are again passed into the discriminator. But in this case the generator only sees its own fake examples. So it doesn't see the real examples at all. So it only knows that these are being passed to some discriminator. And then the discriminator makes predictions Y hat of how real or fake these are. And after that the predictions are compared using the BCE loss with all the labels equal to real. Because the generator is trying to get these fake images to be equal to real or label of 1 as closely as possible. And so this is where it's a little bit tricky and a little bit different between the generator and discriminator. Discriminator wants the fake examples to seem as fake as possible, but the generator wants fake examples to seem as real as possible. That is, it wants to fool the discriminator. And so, after computing the cost, the gradient is then propagated backwards and the parameters of the generator or theta sub g are updated.  it's only the generator, this one neural network that is getting updated in this process, not the discriminator.

So as you alternate their training, only one model is trained at a time, while the other one is held constant. So in training GANs in this alternating fashion, it's important to keep in mind that both models should improve together and should be kept at similar skill levels from the beginning of training. And so the reasoning behind this is if you had a discriminator that is superior than the generator, like super, super good, you'll get predictions from it telling you that all the fake examples are 100% fake. Well, that's not useful for the generator, the generator doesn't know how to improve. Everything just looks super fake, there isn't anything telling it to know which direction to go in. Maybe to add something a little bit more realistic and how to learn over time. Meanwhile, if you had a superior generator that completely outskills the discriminator, you'll get predictions telling you that all the generated images are 100% real. So when training GANs in this alternating fashion, it's important to keep in mind that both models should improve together. And should be kept at similar skill levels from the beginning of training. And the reasoning behind this is largely because of the discriminator. The discriminator has a much easier task, it's just trying to figure out which ones are real, which ones are fake, as opposed to model the entire space of what a class could look like. What all cats could look like. And so the discriminator's job is much easier than the generator's. One common issue is having a superior discriminator, having this discriminator learn too quickly. And when it learns too quickly and it suddenly looks at a fake image and says, this is 100% fake, I know this is fake. But this 100% is not useful for the generator at all because it doesn't know which way to grow and learn. And so having output from the discriminator be much more informative, like 0.87 fake or 0.2 fake as opposed to just 100% fake. One, probability one fake, is much more informative to the generator in terms of updating its weights and having it learn to generate realistic images over time.




## Transposed Convolutions and issues
Transposed convolutions are used as an upsampling method and they have learnable parameters unlike, upsampling layers. A problem arising from the use of transposed convolutions, however, is that the output has a checkerboard problem.The output image has a pattern resembling a checkerboard and this arises because when you upsample with a filter, some pixels are influenced much more heavily while the ones around it are not. 

Using upsampling followed by convolution is becoming a more popular technique now to avoid this checkerboard problem.



## Mode Collapse
A mode in a distribution of data is just an area with a high concentration of observations. For instance, the mean value in a normal distribution is the single mode of that distribution, and there's certainly distributions with multiple modes where the mean doesn't have to be one of them. In this distribution, that's bimodal, it has two modes, or multimodal, meaning it has multiple modes. More intuitively, any peak on the probability density distribution over features is a mode of that distribution. 

Remember the example of 1s and 7s was shown in the video tutorial.

Modes are peaks and the probability distribution of our features. Real-world datasets have many modes related to each possible class within them, like the digits in this dataset of handwritten digits. Mode collapse happens when the generator learns to fool the discriminator by producing examples from a single class from the whole training dataset like handwritten number ones. This is unfortunate because, while the generator is optimizing to fool the discriminator, that's not what you ultimately want your generator to do.


## Problems with BCE: mode collapse and vanishing gradient problems

BCE loss function, its just an average of the cost for the discriminator for misclassifying real and fake observations. Where the first term is for reals and the second term is for the fakes. The higher this cost value is, the worse the discriminator is doing at it. The generator wants to maximize this cost because that means the discriminator is doing poorly and is classifying it's fake values into reals. Whereas the discriminator wants to minimize this cost function because that means it's classifying things correctly. Of course the generator only sees the fake side of things, so it actually doesn't see anything about the reals. This maximization and minimization is often called a minimax game, and that's how you might hear it being referred to as. At the end of this minimax game, the generator and discriminator interaction translates to a more general objective for the whole GAN architecture. That is to make the real in generated data distributions of features very similar. Trying to get the generated distribution to be as close as possible to the reals. This minimax of the Binary Cross-Entropy loss function is somewhat approximating the minimization of another complex hash function that's trying to make this happen. Of course, during this whole training process, the discriminator naturally is trying to delineate this real and fake distribution as much as possible, whereas the generator is trying to make the generated distribution look more like the reals. However, let's take a step back again to the generator and discriminators roles. The discriminator and again, needs to output just a single value prediction within zero and one. Whereas the generator actually needs to produce a pretty complex output composed of multiple features to try and fool the discriminator, for example, an image. As a result that discriminators job tends to be a little bit easier. To put it in another way, it's more straightforward to look at images in a museum than it is to paint those masterpieces, right? During training it's possible for the discriminator to outperform the generator, very possible, in fact, quite common. But at the beginning of training, this isn't such a big problem because the discriminator isn't that good. It has trouble distinguishing the generated and real distributions. There's some overlap and it's not quite sure. As a result, it's able to give useful feedback in the form of a non-zero gradient back to the generator. However, as it gets better at training, it starts to delineate the generated and real distributions a little bit more such that it can start distinguishing them much more. Where the real distribution will be centered around one and the generated distribution will start to approach zero. As a result, when it's starting to get better, as this discriminator is getting better, it'll start giving less informative feedback. In fact, it might give gradients closer to zero, and that becomes unhelpful for the generator because then the generator doesn't know how to improve. This is how the vanishing gradient problem will arise. In summary, GANs try to make the generated distribution look similar to the real one by minimizing the underlying cost function that measures how different the distributions are. As a discriminator improves during training and sometimes improves more easily than the generator, that underlying cost function will have those flat regions when the distributions are very different from one another, where the discriminator is able to distinguish between the reals and the fakes much more easily, and be able to say, "Reals look really real, a label of one and fakes look really fake, a label of zero." All of this will cause vanishing gradient problems.


## Earth Mover’s Distance
When using BCE loss to train a GAN, you often encounter mode collapse, and vanishing gradient problems due to the underlying cost function of the whole architecture. Even though there is an infinite number of decimal values between zero and one, the discriminator, as it improves, will be pushing towards those ends. In this video, you'll see a different underlying cost function called Earth mover's distance, that measures the distance between two distributions and generally outperforms the one associated with BCE loss for training GANs. At the end I'll show you why this helps with the vanishing gradient problem. So take this generated and real distributions with the same variance but different means, and assume they might be normal distributions. What the Earth Mover's distance does, is it measures how different these two distributions are, by estimating the amount of effort it takes to make the generated distribution equal to the real. So intuitively, the generate distribution was a pile of dirt, how difficult would it be to move that pile of dirt and mold it into the shape and location of the real distribution? So that's what this Earth mover's distance means. The function depends on both the distance and the amount that the generated distribution needs to be moved. I'll show you a function that calculates this in the next video. So the problem with BCE loss is that as a discriminator improves, it would start giving more extreme values between zero and one, so values closer to one and closer to zero. As a result, this became less helpful feedback back to the generator. So the generator would stop learning due to vanishing gradient problems. With Earth mover's distance, however, there's no such ceiling to the zero and one. So the cost function continues to grow regardless of how far apart these distributions are. The gradient of this measure won't approach zero and as a result, GANs are less prone to vanishing gradient problems and from vanishing gradient problems, mode collapse. So wrapping up, Earth mover's distance is a function of the effort to make a distribution equal to another. So it depends on both distance and amount. Unlike BCE, it doesn't have flat regions when the distributions start to get very different, and the discriminator starts to improve a lot. So approximating this measure eliminates the vanishing gradient problem, and reduces the likelihood of mode collapse in GANs. 


