## Discriminator
The discriminator is a type of classifier that learns to model the probability of an example being real or fake given that set of input features, like RGB pixel values for images.The output probabilities from the discriminator are the ones that help the generator learn to produce better looking examples overtime.


<img src="https://render.githubusercontent.com/render/math?math=P(y | X)"> or <img src="https://render.githubusercontent.com/render/math?math=P(class | features)">

The y(class) can be “real” or “fake” here. X is an image generated by Generator.


## Generators
The generator produces fake data that tries to look real. It learns to mimic that distribution of features X from the class of your data. In order to produce different outputs each time it takes random features(noise) as input.


<img src="https://render.githubusercontent.com/render/math?math=P(X | y)"> or <img src="https://render.githubusercontent.com/render/math?math=P(features | class)">

If we have only one class(y) , then the second term in the conditional probability will be ignored(coz it will be same always) and we will only have P(X). If we want to model the probabilities across all classes then we will have to put the second term.


The more common features(in the dataset) will be appearing more, and rare features will appear rare.

## Binary CrossEntropy(BCE)
The BCE cost function has two main terms that are relevant for each of the classes. Whether prediction and the label are similar, the BCE loss is close to 0. When they're very different, that BCE loss approaches infinity. The BCE loss is performed across a mini-batch of several examples, let say n examples, maybe five examples where n equals 5. It takes the average of all those five examples. Each of those examples can be different. One of them can be 1, the other four could be 0, for their different classes.


## training GANs : Discriminator
So to train a basic GAN, you alternate the training on the generator and the discriminator. So first of course you get some fake examples X hat produced by the generator from that input noise. And then those examples, the fake ones, X hat and real ones X are both passed into the discriminator without telling the discriminator just yet which ones are real and which ones are fake. And then the discriminator makes predictions Y hat of which ones are real and which ones are fake. Or more specifically a probability of score of how fake and how real each of these images are. After that, the predictions are compared using that BCE loss with the desired labels for fake and real. And that helps update its parameters or theta d, where d represents parameters for the discriminator. And so this only updates the parameters of the discriminator, only this one neural network, and not the generator.

## training GANs : Generator
so for the generator it first generates a few fake examples, again X hat, and this is from the input noise. And then these are again passed into the discriminator. But in this case the generator only sees its own fake examples. So it doesn't see the real examples at all. So it only knows that these are being passed to some discriminator. And then the discriminator makes predictions Y hat of how real or fake these are. And after that the predictions are compared using the BCE loss with all the labels equal to real. Because the generator is trying to get these fake images to be equal to real or label of 1 as closely as possible. And so this is where it's a little bit tricky and a little bit different between the generator and discriminator. Discriminator wants the fake examples to seem as fake as possible, but the generator wants fake examples to seem as real as possible. That is, it wants to fool the discriminator. And so, after computing the cost, the gradient is then propagated backwards and the parameters of the generator or theta sub g are updated.  it's only the generator, this one neural network that is getting updated in this process, not the discriminator.

So as you alternate their training, only one model is trained at a time, while the other one is held constant. So in training GANs in this alternating fashion, it's important to keep in mind that both models should improve together and should be kept at similar skill levels from the beginning of training. And so the reasoning behind this is if you had a discriminator that is superior than the generator, like super, super good, you'll get predictions from it telling you that all the fake examples are 100% fake. Well, that's not useful for the generator, the generator doesn't know how to improve. Everything just looks super fake, there isn't anything telling it to know which direction to go in. Maybe to add something a little bit more realistic and how to learn over time. Meanwhile, if you had a superior generator that completely outskills the discriminator, you'll get predictions telling you that all the generated images are 100% real. So when training GANs in this alternating fashion, it's important to keep in mind that both models should improve together. And should be kept at similar skill levels from the beginning of training. And the reasoning behind this is largely because of the discriminator. The discriminator has a much easier task, it's just trying to figure out which ones are real, which ones are fake, as opposed to model the entire space of what a class could look like. What all cats could look like. And so the discriminator's job is much easier than the generator's. One common issue is having a superior discriminator, having this discriminator learn too quickly. And when it learns too quickly and it suddenly looks at a fake image and says, this is 100% fake, I know this is fake. But this 100% is not useful for the generator at all because it doesn't know which way to grow and learn. And so having output from the discriminator be much more informative, like 0.87 fake or 0.2 fake as opposed to just 100% fake. One, probability one fake, is much more informative to the generator in terms of updating its weights and having it learn to generate realistic images over time.




## Transposed Convolutions and issues
Transposed convolutions are used as an upsampling method and they have learnable parameters unlike, upsampling layers. A problem arising from the use of transposed convolutions, however, is that the output has a checkerboard problem.The output image has a pattern resembling a checkerboard and this arises because when you upsample with a filter, some pixels are influenced much more heavily while the ones around it are not. 

Using upsampling followed by convolution is becoming a more popular technique now to avoid this checkerboard problem.



## Mode Collapse
A mode in a distribution of data is just an area with a high concentration of observations. For instance, the mean value in a normal distribution is the single mode of that distribution, and there's certainly distributions with multiple modes where the mean doesn't have to be one of them. In this distribution, that's bimodal, it has two modes, or multimodal, meaning it has multiple modes. More intuitively, any peak on the probability density distribution over features is a mode of that distribution. 

Remember the example of 1s and 7s was shown in the video tutorial.

Modes are peaks and the probability distribution of our features. Real-world datasets have many modes related to each possible class within them, like the digits in this dataset of handwritten digits. Mode collapse happens when the generator learns to fool the discriminator by producing examples from a single class from the whole training dataset like handwritten number ones. This is unfortunate because, while the generator is optimizing to fool the discriminator, that's not what you ultimately want your generator to do.


## Problems with BCE: mode collapse and vanishing gradient problems

BCE loss function, its just an average of the cost for the discriminator for misclassifying real and fake observations. Where the first term is for reals and the second term is for the fakes. The higher this cost value is, the worse the discriminator is doing at it. The generator wants to maximize this cost because that means the discriminator is doing poorly and is classifying it's fake values into reals. Whereas the discriminator wants to minimize this cost function because that means it's classifying things correctly. Of course the generator only sees the fake side of things, so it actually doesn't see anything about the reals. This maximization and minimization is often called a minimax game, and that's how you might hear it being referred to as. At the end of this minimax game, the generator and discriminator interaction translates to a more general objective for the whole GAN architecture. That is to make the real in generated data distributions of features very similar. Trying to get the generated distribution to be as close as possible to the reals. This minimax of the Binary Cross-Entropy loss function is somewhat approximating the minimization of another complex hash function that's trying to make this happen. Of course, during this whole training process, the discriminator naturally is trying to delineate this real and fake distribution as much as possible, whereas the generator is trying to make the generated distribution look more like the reals. However, let's take a step back again to the generator and discriminators roles. The discriminator and again, needs to output just a single value prediction within zero and one. Whereas the generator actually needs to produce a pretty complex output composed of multiple features to try and fool the discriminator, for example, an image. As a result that discriminators job tends to be a little bit easier. To put it in another way, it's more straightforward to look at images in a museum than it is to paint those masterpieces, right? During training it's possible for the discriminator to outperform the generator, very possible, in fact, quite common. But at the beginning of training, this isn't such a big problem because the discriminator isn't that good. It has trouble distinguishing the generated and real distributions. There's some overlap and it's not quite sure. As a result, it's able to give useful feedback in the form of a non-zero gradient back to the generator. However, as it gets better at training, it starts to delineate the generated and real distributions a little bit more such that it can start distinguishing them much more. Where the real distribution will be centered around one and the generated distribution will start to approach zero. As a result, when it's starting to get better, as this discriminator is getting better, it'll start giving less informative feedback. In fact, it might give gradients closer to zero, and that becomes unhelpful for the generator because then the generator doesn't know how to improve. This is how the vanishing gradient problem will arise. In summary, GANs try to make the generated distribution look similar to the real one by minimizing the underlying cost function that measures how different the distributions are. As a discriminator improves during training and sometimes improves more easily than the generator, that underlying cost function will have those flat regions when the distributions are very different from one another, where the discriminator is able to distinguish between the reals and the fakes much more easily, and be able to say, "Reals look really real, a label of one and fakes look really fake, a label of zero." All of this will cause vanishing gradient problems.


## Earth Mover’s Distance (EMD)
When using BCE loss to train a GAN, you often encounter mode collapse, and vanishing gradient problems due to the underlying cost function of the whole architecture. Even though there is an infinite number of decimal values between zero and one, the discriminator, as it improves, will be pushing towards those ends. In this video, you'll see a different underlying cost function called Earth mover's distance, that measures the distance between two distributions and generally outperforms the one associated with BCE loss for training GANs. At the end I'll show you why this helps with the vanishing gradient problem. So take this generated and real distributions with the same variance but different means, and assume they might be normal distributions. What the Earth Mover's distance does, is it measures how different these two distributions are, by estimating the amount of effort it takes to make the generated distribution equal to the real. So intuitively, the generate distribution was a pile of dirt, how difficult would it be to move that pile of dirt and mold it into the shape and location of the real distribution? So that's what this Earth mover's distance means. The function depends on both the distance and the amount that the generated distribution needs to be moved. I'll show you a function that calculates this in the next video. So the problem with BCE loss is that as a discriminator improves, it would start giving more extreme values between zero and one, so values closer to one and closer to zero. As a result, this became less helpful feedback back to the generator. So the generator would stop learning due to vanishing gradient problems. With Earth mover's distance, however, there's no such ceiling to the zero and one. So the cost function continues to grow regardless of how far apart these distributions are. The gradient of this measure won't approach zero and as a result, GANs are less prone to vanishing gradient problems and from vanishing gradient problems, mode collapse. So wrapping up, Earth mover's distance is a function of the effort to make a distribution equal to another. So it depends on both distance and amount. Unlike BCE, it doesn't have flat regions when the distributions start to get very different, and the discriminator starts to improve a lot. So approximating this measure eliminates the vanishing gradient problem, and reduces the likelihood of mode collapse in GANs. 

## Wasserstein Loss or W-Loss 
BCE Loss is computed by a long equation that essentially measures how bad, on average, some observations are being classified by the discriminator, as fake and real.

So, the generator in GANs wants to maximize this cost, because that means the discriminator is saying that its fake values seem really real, while the discriminator wants to minimize that cost.


And so, this is often referred to as a Minimax game. And this very long equation for BCE Loss can be simplified as follows. The sum and division over examples M is nothing but a mean or expected value. In the first part, inside the sum, measures how bad the discriminator classifies real observations, where y equals 1, and 1 means real. And the second part measures how bad it classifies fake observations produced by the generator, where y of 1 means real, but 1 minus y, y of 0, means fake. W-Loss, on the other hand, approximates the Earth Mover's Distance between the real and generated distributions, but it has nicer properties than BCE. However, it does look very similar to the simplified form for the BCE Loss, and in this case the function calculates the difference between the expected values of the predictions of the discriminator. Here it's called the critic, and I'll go over that later, so I'm going to represent it with a c here. And this is c of a real example x, versus C of a fake example g of z. Generator taking in a noise vector to produce a fake image g of z, or perhaps you can call it x-hat.

So the discriminator looks at these two things, and it wants to maximize the distance between its thoughts on the reals versus its thoughts on the fakes. So it's trying to push away these two distributions to be as far apart as possible. Meanwhile, the generator wants to minimize this difference, because it wants the discriminator to think that its fake images are as close as possible to the reals. I know that in contrast with BCE there are no logs in this function, since the critics outputs are no longer bounded to be between 0 and 1.


So, for the BCE Loss to make sense, the output of the discriminator needs to be a prediction between 0 and 1. And so the discriminator's neural network for GANs, trained with BCE Loss, have a sigmoid activation function in the output layer to then squash the values between 0 and 1. W-Loss, however, doesn't have that requirement at all, so you can actually have a linear layer at the end of the discriminator's neural network, and that could produce any real value output. And you can interpret that output as, how real an image is considered by the critic, which, by the way, is now what we're calling the discriminator instead, because it's no longer bounded between 0 and 1, where 0 means fake, and 1 means real. It's no longer classifying into these two, or discriminating between these two classes. And so, as a result, it wouldn't make that much sense to call that neural network a discriminator, because it doesn't discriminate between the classes. And so, for W-Loss, the equivalent to a discriminator is called a critic, and what it tries to do is, maximize the distance between its evaluation on a fake, and its evaluation on a real.


So, some of the main differences between W-Loss and BCE Loss is that, the discriminator under BCE Loss outputs a value between 0 and 1, while the critic in W-Loss will output any number. Additionally, the forms of the cost functions is very similar, but W-Loss doesn't have any logarithms within it, and that's because it's a measure of how far the prediction of the critic for the real is from its prediction on the fake. Meanwhile, BCE Loss does measure that distance between fake or a real, but to a ground truth of 1 or 0. And so what's important to take away here is largely that, the discriminator is bounded between 0 and 1, whereas the critic is no longer bounded ,and just trying to separate the two distributions as much as possible. And as a result, because it's not bounded, the critic is allowed to improve without degrading its feedback back to the generator. And this is because, it doesn't have a vanishing gradient problem, and this will mitigate against mode collapse, because the generator will always get useful feedback back. So, in summary, W-Loss looks very similar to BCE Loss, but it isn't as complex a mathematical expression. Under the hood what it does is, approximates the Earth Mover's Distance, so it prevents mode collapse in vanishing gradient problems. However, there is an additional condition on this cost function for it to work well and for it to be valid.

<img src="https://render.githubusercontent.com/render/math?math=min_g max_c E(c(x)) - E(c(g(z)))"> 

or 


<img src="https://render.githubusercontent.com/render/math?math=min_g max_c Expectation(critic(image)) - Expectation(critic(generator(noise)))"> 


## Condition on Wasserstein Critic : 1-Lipschitz Continuous
W-Loss is a simple expression that computes the difference between the expected values of the critics output for the real examples x and its predictions on the fake examples g(z). The generator tries to minimize this expression, trying to get the generative examples to be as close as possible to the real examples while the critic wants to maximize this expression because it wants to differentiate between the reals and the fakes, it wants the distance to be as large as possible. However, for training GANs using W-Loss, the critic has a special condition. It needs to be something called 1-Lipschitz Continuous or 1-L Continuous for short. This condition sounds more sophisticated than it really is. For a function like the critics neural network to be at 1-Lipschitz Continuous, the norm of its gradient needs to be at most one. What that means is that, the slope can't be greater than one at any point, its gradient can't be greater than one. To check if a function here, for example, this function you see here, f(x) equals x squared, is 1-Lipschitz Continuous, you want to go along every point in this function and make sure its slope is less than or equal to one, or its gradient is less than or equal to one, and what you can do is, you can actually draw two lines, one where the slope is exactly one at this certain point that you're evaluating function, and one where the slope is negative one where you're evaluating our function. You want to make sure that the growth of this function never goes out of bounds from these lines because staying within these lines means that the function is growing linearly. Here this function is not Lipschitz Continuous because it's coming out in all these sections. It's not staying within this green area, which suggests that it's growing more than linearly. Look at another example here. This is a smooth curve functions. You want to again check every single point on this function before you can determine whether or not that this is 1-Lipschitz Continuous. Here it looks fine, function looks good. Here it also looks good, here looks good. Let's say you take every single value and the function never grows more than linearly. This function is 1-Lipschitz Continuous. This condition on the critics neural network is important for W-Loss because it assures that the W-Loss function is not only continuous and differentiable, but also that it doesn't grow too much and maintain some stability during training. This is what makes the underlying Earth Movers Distance valid, which is what W-Loss is founded on. This is required for training both the critic and generators neural networks and it also increases stability because the variation as the GAN learns will be bounded. To recap, the critic, and again that uses W-Loss for training needs to be 1-Lipschitz Continuous in order for its underlying Earth Mover's Distance comparison between the reals and the fakes to be a valid comparison. In order to satisfy or try to satisfy this condition during training, there are multiple different methods. 


## 1-Lipschitz Continuity Enforcement

One Lipschitz continuity or 1-L continuity of the critic neural network in your Wasserstein loss and gain ensures that Wasserstein loss is valid. You already saw what this means and this video, I'll show you how to enforce this condition when training your critic. First, I'll introduce you to two different methods to enforce 1-L continuity on the critic, namely weight clipping and gradient penalty. Then I'll discuss the advantages of gradient penalty over weight clipping. First recall that the critic being 1-L continuous means that the norm of its gradient is at most one at every single point of this function. This upside down triangle is assigned for gradient, this is the function, perhaps F is the critic here and X is the image. This just represents the norm of that gradient being less than or equal to one. Using the L2 norm is very common here, which just means its Euclidean distance or often thought of as your triangle distance of your hypotenuse. This is the distance between these two points not going this direction. It's this hypotenuse. Intuitively in two-dimensions, it's that the slope is less than or equal to one. At every single point of this function, it'll remain within these green triangles. Two common ways of ensuring this condition are weight clipping and gradient penalty. 

### 1-Lipschitz Enforcement : weight clipping
With weight clipping, the weights of the critics neural network are forced to take values between a fixed interval. After you update the weights during gradient descent, you actually will clip any weights outside of the desired interval. Basically what that means is that weights over that interval, either too high or too low, will be set to the maximum or the minimum amount allowed. That's clipping the weights there. This is one way of enforcing the 1-L continuity, but it has a way to downside. Forcing the weights of the critic to a limited range of values could limit the critics ability to learn and ultimately for the gradient to perform because if the critic can't take on many different parameter values, it's weights can't take on many different values, it might not be able to improve easily or find good loop optimal for it to be in. Not only is this trying to do 1-L continuity enforcement, this might also limit the critic too much. Or on the other hand, it might actually limit the critic too little if you don't clip the weights enough. There's a lot of hyperparameter tuning involved.

### 1-Lipschitz Enforcement : gradient penalty
The gradient penalty, which is another method, is a much softer way to enforce the critic to be one lipschitz continuous. With the gradient penalty, all you need to do is add a regularization term to your loss function. What this regularization term does to your W loss function, is that it penalizes the critic when it's gradient norm is higher than one. I'll dive into what that means. The regularization term is as reg here, which I'll unfold shortly. Lambda is just a hyperparameter value of how much to weigh this regularization term against the main loss function. In order to check the critics gradient at every possible point of the feature space, that's virtually impossible or at least not practical. Instead with gradient penalty during implementation, of course, all you do is sample some points by interpolating between real and fake examples. For instance, you could sample an image with a set of reals and an image of the set of fakes, and you grab one of each and you can get an intermediate image by interpolating those two images using a random number epsilon. Epsilon here it could be a weight of 0.3, and here it would evaluate one minus epsilon would be 0.7. That would get you this random interpolated image that's in-between these two images. I'll call this random interpolated image X hat. It's on X hat that you want to get the critics gradient to be less than or equal to one. This is exactly what's happening here. The critic looks at X hat, you get the gradient of the critics prediction on X hat, and then you take the norm of that gradient and you want the norm to be one. Here it's simpler to say, "Hey, can I get the norm of the gradient to be one as opposed to at most one?" Because this in fact is penalizing any value outside of one. The two here is just saying,"I want the squared distance as opposed to perhaps the absolute value between them, penalizing values much more when they're further away from one." Specifically, that X hat is an intermediate image where it's weighted against the real and a fake using epsilon. With this method, you're not strictly enforcing 1-L continuity, but you're just encouraging it. This has proven to work well and much better than weight clipping. The complete expression, the loss function that you use for training again with W loss ingredient penalty now has these two components. First, you approximate Earth Mover's distance with this main W loss component. This makes again less parental mode collapse and managed ingredients. The second part of this loss function is a regularization term that meets the condition for what the critic desires in order to make this main term valid. Of course, this is a soft constraint on making the critic one lipschitz continuous, but it has been shown to be very effective. Keeping the norm of the critic close to one almost everywhere is actually the technical term is almost anywhere. Wrapping up in this video, I presented you with two ways of enforcing the critic to be one lipschitz continuous or 1L continuous, weight clipping as one and ingredient penalty as the other. Weight clipping might be problematic because you're strongly limiting the way the critic learns during training or you're being too soft, so there's a bit of hyperparameter tuning. Gradient penalty on the other hand, is a softer way to enforce one Lipschitz continuity. While it doesn't strictly enforce the critics gradient norm to be less than one at every point, it works better in practice than weight clipping.

## Conditional Generation
With conditional generation, you can get generated examples from the classes you decide while with unconditional generation, you get examples from a random class. As a result of that, with conditional generation, you actually have to train your GAN with labeled datasets and those labels are on the different classes you want while unconditional generation doesn't need any labels. You've seen this in previous weeks from the course, that you don't need any labels you just need a pile of real examples. You see how to modify your model for this conditional generation in the following lectures. What you should take away from this video is that conditional generation requires labeled datasets for training in order to learn how to produce examples from desired classes. Coming up, I'll show you how the labels from your dataset are fed to the generator and discriminator in order to train your GAN and produce examples from the desired class, like selecting the red soda from a vending machine.




## Conditional Generation: Inputs
You've seen already with unconditional generation, the generator needs a noise vector to produce random examples. For conditional generation, you also need a vector to tell the generator from which class the generated examples should come from. Usually this is a one-hot vector, which means that there are zeros in every position except for one position corresponding to the class you want. Remember that the noise vector is random values as well, but it doesn't have to zero or one.

To sum up in conditional generation, you pass the class information to both models. To the generator it's typically a one-hot vector concatenated with your noise vector to the discriminator when the desired output of the GANs are images, it's one-hot matrices representing the channels. The size of the class vector and the number of extra channels for the class information is just the same as the number of classes you'll be turning on.





## Controllable Generation
With controllable generation, you're able to get examples with the features that you want, like faces from people who look older with green hair and glasses. With conditional generation, you get examples from the class that you want, like a human or bird. Of course, it could also be, I want to person with sunglasses on as well. So far, they're a bit similar. But controllable generation typically means you want to control how much or how little of a feature you want. They're typically more continuous features like age. Conditional generation on the other hand, allows you to specify what class you want to a very different type of thing. For this, you need to have a label data set and implemented during training typically. You probably don't want to label every hair length value, so, controllable generation will do that for you and it's more about finding directions of the features you want. That can happen after training. Of course, controllable generation, you will sometimes also see it happening during training as well. To help nudge the model in a direction where it's easier to control. Finally, as you just learned, controllable generation works by tweaking that input noise vector Z that's fed into the generator, while with conditional generation, you have to pass additional information representing the class that you want appended to that noise vector. 

In summary. Controllable generation lets you control the features in the output from your gan. In contrast, with conditional generation, there's no need for a labeled training dataset. To change the output in some way with controllable generation, the input noise vector is tweaked in some other way. In following videos, I'll dig deeper into how exactly that works.



## Vector Algebra in the Z-Space

Controllable generation and interpolation are somewhat alike. With interpolation, you get these intermediate examples between two generated observations. In practice, with interpolation, you can see how an image morphs into another, like in this GIF, where each digit from zero to nine morphs into the following one. It's pretty cool. What happens is that you get intermediate examples between the targets by manipulating the inputs from Z-space, which is just the name for the vector space of the noise vectors. You'll see later that this is the same idea behind control bot generation. Just to be clear here, Z_1 and Z_2 are the two dimensions in this Z-space that you're looking at right now. As an example, there's a noise vector V_1 and a noise vector V_2, where V_1 could have a Z_1 value of, let's say five and a Z_2 value of let's say 10. Then this is the vector 5,10 and then V_2 has a smaller value, so four and two. So it's the vector 4, 2. That's what Z_1 and Z_2 are, just dimensions on the Z-space and the actual vectors V_1 and V_2 are going to represent concrete vector values in this Z-space. V_1, when you feed it into the generator, will produce this image here, and V_2 when you feed it into the generator, will produce this image there. If you want to get intermediate values between these two images, you can make an interpolation between their two input vectors, V_1 and V_2 in the Z-space, actually. This interpolation is often a linear interpolation. Of course, there are other ways to interpolate between these two vectors. Then you can take all these intermediate vectors and see what they produce from the generator. The generator takes this vector and produces that image, this vector, that image, and this vector, that image to get this gradient between these two images. Controllable generation also uses changes in the Z-space and takes advantage of how modifications to the noise vectors are reflected on the output from the generator. For example, with the noise vector, you could get a picture of a woman with red hair and then with another noise vector, you could get a picture of the same woman but with blue hair. The difference between these two noise vectors is just this direction in which you have to move in Z-space to modify the hair color of your generated images. In controllable generation, your goal is to find these directions for different features you care about. For example, modifying hair color. But don't worry about finding that exact direction yet, I'm going to show you in the following lectures. With this known direction d, let's call this direction d, in your Z-space, you can now control the features on the output of your GAN, which is really exciting. This means that if you then generate an image of a man with red hair produced by the same Generator g, with this input noise vector here, V_1, you can modify the hair color of this man in the image by adding that direction vector d you found earlier to the noise vector, creating this new noise vector here, V_1 + d, passing that into your generator and getting a resulting image where his hair is now blue. 

To sum up, in controllable generation, you need to find the directions in the Z-space related to changes of the desired features on the output of your GAN. With known directions, controllable generation works by moving the noise vector in different directions in that Z-space. Up next, you'll learn some challenges related to controllable generation and how to find directions on the Z-space with known effects on the generated outputs.



## Challenges with Controllable Generation
Controllable generation makes it so that you can decide the features and the output of a GAN, like hair color or hair length in the GAN that produces pictures of people. However, controllable generation has a couple of challenges that you'll see in this video. Specifically, you'll learn about feature correlation and the alpha space in Z space entanglement. When different features have a high correlation in the data set, they use to train your GAN, it becomes difficult to control specific features without modifying the ones that are correlated to them. For example, ideally you want to be able to control single features like the amount of beard on a person's face produced by your GAN and if features in the data set don't happen to have a high correlation, you'd be able to take this picture of a woman and add a beard to her by moving in some direction in Z space. However, it's very likely that in the data set you use for training features like the presence of a beard and how masculine the face looks will be strongly correlated. If you want to add a beard to the picture of a woman, you'd end up modifying many more features on the output. But that perhaps is not desirable because you want to be able to find these directions where you can just change one feature about someone. That way you can reliably edit images. Another challenge faced by controllable generation is known as entanglement in the Z space. When the Z space is entangled, movement in the different directions has an effect on multiple features simultaneously in the output. Even if those features aren't necessarily correlated in your training data set. This is just how the noise space was learned; to be very entangled. In this entangled Z space, when you control if a person in the output has glasses, for example, you also end up modifying whether she has a beard in her hair or when you try to modify her apparent age, you'll end up also changing her apparent eyes and hair color too. That's not quite desirable. The same happens with other uncorrelated features. This means that changes in some of the components of the noise vectors change multiple features in the output at the same time and this makes it difficult, if not impossible, to control the output. This is a very common problem when the Z space doesn't have enough dimensions relative to the number of features you want to control in the output because then it actually can't map things one-to-one. This is also just generally an issue when training generative models. 

To recap, controllable generation faces several challenges. If features in your data set has a high correlation with each other and you don't account for this in some way, then when you try to control the output of your GAN, you end up changing multiple features at a time. Even if the features you want to control don't have a high correlation with each other in the training data set, control of a generation is also difficult if your Z space is entangled. Which will happen commonly if the number of dimensions in your Z space is not large enough but there are a host of other reasons of why that happens as well.


## Classifier Gradients

Trouble generation works by moving into the z-space, according to directions that correspond to desired features, such as lengthening hair or shortening hair. In this video, you'll learn a very simple method used to find that direction using the gradient of trained classifiers, you'll see what the requirements are for this method. So to find the direction in the z-space that modifies certain features on the output, say the presence of sunglasses. You could use a train classifier that identifies if a person in a picture has that said feature. So to do so, you could take a batch of noise vector Z, that goes through the generator to get some images. You then pass these images through a sunglasses classifier, which will tell you at the outputs correspond to people with or without sunglasses. Then you use them information to modify your Z vectors, and this is without modifying the weights of the generator at all. So the generator weights are frozen. You're done training, and you modify your Z vectors by moving in the direction of the gradient with the costs. That penalizes the model for every image classified as not having sunglasses. So then you repeat this process until the images are classified as people with sunglasses.


So this method is very simply inefficient, and some might argue, even lazy, because you're using this classifier that's already there for you. But I think it's great, that you're taking advantage of a pre-training classifier. However, of course there's also the downside of that. You need a pre-trained classifier that accurately detects the feature that you want to control with four hand. You can also train your own of course, so if you wanted to classifier that detected beards, for example, you might have to train that on your own if that isn't available off the shelf. Be you should always check if something is available out there, because this could be a really simple and cool way to allow you to start controlling your. So you should take away from this video that preacher classifiers can be used to find directions in the z-space associated with features in the output of. And to find those directions using the gradients of the classifiers, you need to modify the noise vectors without changing the generator. So remember all of this happens after training.


## Disentanglement

